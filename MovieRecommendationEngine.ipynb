{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lalithakre/MovieRecommendationEngine/blob/main/MovieRecommendationEngine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGLwTKHESgbS",
        "outputId": "da8e0f23-65ad-422e-f2ec-506b84aa3cc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Movie Recommendation Engine\n"
          ]
        }
      ],
      "source": [
        "print(\"Movie Recommendation Engine\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uguOUa6UaDi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "id": "WB6M4SBHSpTz",
        "outputId": "60b0b1dd-c970-4980-8ed9-ac5084d28564"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-78c72e62-03bd-47e5-b5fd-a00ca3e897cf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-78c72e62-03bd-47e5-b5fd-a00ca3e897cf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "df1=files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFBD71wPT9un"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "df21=files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dt4-pEw8UTed"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "df1\n",
        "The first dataset contains the following features:-\n",
        "movie_id - A unique identifier for each movie.\n",
        "cast - The name of lead and supporting actors.\n",
        "crew - The name of Director, Editor, Composer, Writer etc.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIHWnKOhVJEq"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "df2\n",
        "The second dataset has the following features:-\n",
        "\n",
        "budget - The budget in which the movie was made.\n",
        "genre - The genre of the movie, Action, Comedy ,Thriller etc.\n",
        "homepage - A link to the homepage of the movie.\n",
        "id - This is infact the movie_id as in the first dataset.\n",
        "keywords - The keywords or tags related to the movie.\n",
        "original_language - The language in which the movie was made.\n",
        "original_title - The title of the movie before translation or adaptation.\n",
        "overview - A brief description of the movie.\n",
        "popularity - A numeric quantity specifying the movie popularity.\n",
        "production_companies - The production house of the movie.\n",
        "production_countries - The country in which it was produced.\n",
        "release_date - The date on which it was released.\n",
        "revenue - The worldwide revenue generated by the movie.\n",
        "runtime - The running time of the movie in minutes.\n",
        "status - \"Released\" or \"Rumored\".\n",
        "tagline - Movie's tagline.\n",
        "title - Title of the movie.\n",
        "vote_average - average ratings the movie recieved.\n",
        "vote_count - the count of votes recieved.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHHwjTfNVXUZ"
      },
      "outputs": [],
      "source": [
        "df1.columns = ['id','tittle','cast','crew']\n",
        "df2= df2.merge(df1,on='id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sgDvMAxVXyt"
      },
      "outputs": [],
      "source": [
        "df2.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_elx19nOVcVt"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "we need a metric to score or rate movie\n",
        "Calculate the score for every movie\n",
        "Sort the scores and recommend the best rated movie to the users.\n",
        "We can use the average ratings of the movie as the score but using this won't be fair enough since a movie with 8.9 average rating and only 3 votes cannot be considered better than the movie with 7.8 as as average rating but 40 votes. So, I'll be using IMDB's weighted rating (wr) which is given as :-\n",
        "\n",
        "where,\n",
        "\n",
        "v is the number of votes for the movie;\n",
        "m is the minimum votes required to be listed in the chart;\n",
        "R is the average rating of the movie; And\n",
        "C is the mean vote across the whole report\n",
        "We already have v(vote_count) and R (vote_average) and C can be calculated as\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-_frw6LVnxk"
      },
      "outputs": [],
      "source": [
        "C= df2['vote_average'].mean()\n",
        "C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igevf1HpVpvK"
      },
      "outputs": [],
      "source": [
        "m= df2['vote_count'].quantile(0.9)\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Jh1x410VrrO"
      },
      "outputs": [],
      "source": [
        "q_movies = df2.copy().loc[df2['vote_count'] >= m]\n",
        "q_movies.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z40dToG1Vtdd"
      },
      "outputs": [],
      "source": [
        "def weighted_rating(x, m=m, C=C):\n",
        "    v = x['vote_count']\n",
        "    R = x['vote_average']\n",
        "    # Calculation based on the IMDB formula\n",
        "    return (v/(v+m) * R) + (m/(m+v) * C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EBPLFWuVva9"
      },
      "outputs": [],
      "source": [
        "q_movies['score'] = q_movies.apply(weighted_rating, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1fbyhHmVxC-"
      },
      "outputs": [],
      "source": [
        "q_movies = q_movies.sort_values('score', ascending=False)\n",
        "\n",
        "#Print the top 15 movies\n",
        "q_movies[['title', 'vote_count', 'vote_average', 'score']].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s87LFyxuVy8G"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        " Under the Trending Now tab of these systems we find movies that are very popular and they can just be obtained by sorting the dataset by the popularity column.\n",
        " \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-zu1DZDV-rb"
      },
      "outputs": [],
      "source": [
        "pop= df2.sort_values('popularity', ascending=False)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12,4))\n",
        "\n",
        "plt.barh(pop['title'].head(6),pop['popularity'].head(6), align='center',\n",
        "        color='skyblue')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel(\"Popularity\")\n",
        "plt.title(\"Popular Movies\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EATyOpvzWANP"
      },
      "outputs": [],
      "source": [
        "# Content Based Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pymZTwdkWIKl"
      },
      "outputs": [],
      "source": [
        "df2['overview'].head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaI69XCkWMTb"
      },
      "outputs": [],
      "source": [
        "#Import TfIdfVectorizer from scikit-learn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "#Replace NaN with an empty string\n",
        "df2['overview'] = df2['overview'].fillna('')\n",
        "\n",
        "#Construct the required TF-IDF matrix by fitting and transforming the data\n",
        "tfidf_matrix = tfidf.fit_transform(df2['overview'])\n",
        "\n",
        "#Output the shape of tfidf_matrix\n",
        "tfidf_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDHHOQ6aWQjz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "We see that over 20,000 different words were used to describe the 4800 movies in our dataset.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDeAp8WhWZ2G"
      },
      "outputs": [],
      "source": [
        "# Import linear_kernel\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "# Compute the cosine similarity matrix\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l23M9NnWaL2"
      },
      "outputs": [],
      "source": [
        "#Construct a reverse map of indices and movie titles\n",
        "indices = pd.Series(df2.index, index=df2['title']).drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Julh_O8OWcbk"
      },
      "outputs": [],
      "source": [
        "# Function that takes in movie title as input and outputs most similar movies\n",
        "def get_recommendations(title, cosine_sim=cosine_sim):\n",
        "    # Get the index of the movie that matches the title\n",
        "    idx = indices[title]\n",
        "\n",
        "    # Get the pairwsie similarity scores of all movies with that movie\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "\n",
        "    # Sort the movies based on the similarity scores\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get the scores of the 10 most similar movies\n",
        "    sim_scores = sim_scores[1:11]\n",
        "\n",
        "    # Get the movie indices\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "    # Return the top 10 most similar movies\n",
        "    return df2['title'].iloc[movie_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCFmqpMdWf7q"
      },
      "outputs": [],
      "source": [
        "get_recommendations('The Dark Knight Rises')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKb_nP0bWhwu"
      },
      "outputs": [],
      "source": [
        "get_recommendations('The Avengers')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0GC6lVpWkD1"
      },
      "outputs": [],
      "source": [
        "# Credits, Genres and Keywords Based Recommender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt7pX5r5WnK2"
      },
      "outputs": [],
      "source": [
        "# Parse the stringified features into their corresponding python objects\n",
        "from ast import literal_eval\n",
        "\n",
        "features = ['cast', 'crew', 'keywords', 'genres']\n",
        "for feature in features:\n",
        "    df2[feature] = df2[feature].apply(literal_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sxb-HOrnWo9K"
      },
      "outputs": [],
      "source": [
        "# Get the director's name from the crew feature. If director is not listed, return NaN\n",
        "def get_director(x):\n",
        "    for i in x:\n",
        "        if i['job'] == 'Director':\n",
        "            return i['name']\n",
        "    return np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXy24aeJWrhM"
      },
      "outputs": [],
      "source": [
        "# Returns the list top 3 elements or entire list; whichever is more.\n",
        "def get_list(x):\n",
        "    if isinstance(x, list):\n",
        "        names = [i['name'] for i in x]\n",
        "        #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list.\n",
        "        if len(names) > 3:\n",
        "            names = names[:3]\n",
        "        return names\n",
        "\n",
        "    #Return empty list in case of missing/malformed data\n",
        "    return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9KM3tWwWtnI"
      },
      "outputs": [],
      "source": [
        "# Define new director, cast, genres and keywords features that are in a suitable form.\n",
        "df2['director'] = df2['crew'].apply(get_director)\n",
        "\n",
        "features = ['cast', 'keywords', 'genres']\n",
        "for feature in features:\n",
        "    df2[feature] = df2[feature].apply(get_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVn6y951WvU9"
      },
      "outputs": [],
      "source": [
        "# Print the new features of the first 3 films\n",
        "df2[['title', 'cast', 'director', 'keywords', 'genres']].head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfH6Cy4lWxYh"
      },
      "outputs": [],
      "source": [
        "# The next step would be to convert the names and keyword instances into lowercase and strip all the spaces between them. This is done so that our vectorizer doesn't count the Johnny of \"Johnny Depp\" and \"Johnny Galecki\" as the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hE4cyVMyW2B7"
      },
      "outputs": [],
      "source": [
        "# Function to convert all strings to lower case and strip names of spaces\n",
        "def clean_data(x):\n",
        "    if isinstance(x, list):\n",
        "        return [str.lower(i.replace(\" \", \"\")) for i in x]\n",
        "    else:\n",
        "        #Check if director exists. If not, return empty string\n",
        "        if isinstance(x, str):\n",
        "            return str.lower(x.replace(\" \", \"\"))\n",
        "        else:\n",
        "            return ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYdmYBugW32m"
      },
      "outputs": [],
      "source": [
        "# Apply clean_data function to your features.\n",
        "features = ['cast', 'keywords', 'director', 'genres']\n",
        "\n",
        "for feature in features:\n",
        "    df2[feature] = df2[feature].apply(clean_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o7qmjHbW5ZE"
      },
      "outputs": [],
      "source": [
        "def create_soup(x):\n",
        "    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])\n",
        "df2['soup'] = df2.apply(create_soup, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPi1KxI0W7MM"
      },
      "outputs": [],
      "source": [
        "# Import CountVectorizer and create the count matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count = CountVectorizer(stop_words='english')\n",
        "count_matrix = count.fit_transform(df2['soup'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0sn0YOUW8yk"
      },
      "outputs": [],
      "source": [
        "# Compute the Cosine Similarity matrix based on the count_matrix\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "cosine_sim2 = cosine_similarity(count_matrix, count_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgwUs7ODXAAY"
      },
      "outputs": [],
      "source": [
        "# Reset index of our main DataFrame and construct reverse mapping as before\n",
        "df2 = df2.reset_index()\n",
        "indices = pd.Series(df2.index, index=df2['title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9Sbo4kjXEZy"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "We can now reuse our get_recommendations() function by passing in the new cosine_sim2 matrix as your second argument.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBPTCn5XXF-b"
      },
      "outputs": [],
      "source": [
        "get_recommendations('The Dark Knight Rises', cosine_sim2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAniEU1yXIkM"
      },
      "outputs": [],
      "source": [
        "get_recommendations('The Godfather', cosine_sim2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6OX1dmvXMTu"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Collaborative Filtering\n",
        "Our content based engine suffers from some severe limitations. It is only capable of suggesting movies which are close to a certain movie. That is, it is not capable of capturing tastes and providing recommendations across genres.\n",
        "\n",
        "Also, the engine that we built is not really personal in that it doesn't capture the personal tastes and biases of a user. Anyone querying our engine for recommendations based on a movie will receive the same recommendations for that movie, regardless of who she/he is.\n",
        "\n",
        "Therefore, in this section, we will use a technique called Collaborative Filtering to make recommendations to Movie Watchers. It is basically of two types:-\n",
        "\n",
        "User based filtering- These systems recommend products to a user that similar users have liked. For measuring the similarity between two users we can either use pearson correlation or cosine similarity. This filtering technique can be illustrated with an example. In the following matrixes, each row represents a user, while the columns correspond to different movies except the last one which records the similarity between that user and the target user. Each cell represents the rating that the user gives to that movie. Assume user E is the target.\n",
        "Since user A and F do not share any movie ratings in common with user E, their similarities with user E are not defined in Pearson Correlation. Therefore, we only need to consider user B, C, and D. Based on Pearson Correlation, we can compute the following similarity.\n",
        "\n",
        "From the above table we can see that user D is very different from user E as the Pearson Correlation between them is negative. He rated Me Before You higher than his rating average, while user E did the opposite. Now, we can start to fill in the blank for the movies that user E has not rated based on other users.\n",
        "\n",
        "Although computing user-based CF is very simple, it suffers from several problems. One main issue is that usersâ€™ preference can change over time. It indicates that precomputing the matrix based on their neighboring users may lead to bad performance. To tackle this problem, we can apply item-based CF.\n",
        "\n",
        "Item Based Collaborative Filtering - Instead of measuring the similarity between users, the item-based CF recommends items based on their similarity with the items that the target user rated. Likewise, the similarity can be computed with Pearson Correlation or Cosine Similarity. The major difference is that, with item-based collaborative filtering, we fill in the blank vertically, as oppose to the horizontal manner that user-based CF does. The following table shows how to do so for the movie Me Before You.\n",
        "It successfully avoids the problem posed by dynamic user preference as item-based CF is more static. However, several problems remain for this method. First, the main issue is scalability. The computation grows with both the customer and the product. The worst case complexity is O(mn) with m users and n items. In addition, sparsity is another concern. Take a look at the above table again. Although there is only one user that rated both Matrix and Titanic rated, the similarity between them is 1. In extreme cases, we can have millions of users and the similarity between two fairly different movies could be very high simply because they have similar rank for the only user who ranked them both.\n",
        "\n",
        "Single Value Decomposition\n",
        "One way to handle the scalability and sparsity issue created by CF is to leverage a latent factor model to capture the similarity between users and items. Essentially, we want to turn the recommendation problem into an optimization problem. We can view it as how good we are in predicting the rating for items given a user. One common metric is Root Mean Square Error (RMSE). The lower the RMSE, the better the performance.\n",
        "\n",
        "Now talking about latent factor you might be wondering what is it ?It is a broad idea which describes a property or concept that a user or an item have. For instance, for music, latent factor can refer to the genre that the music belongs to. SVD decreases the dimension of the utility matrix by extracting its latent factors. Essentially, we map each user and each item into a latent space with dimension r. Therefore, it helps us better understand the relationship between users and items as they become directly comparable. The below figure illustrates this idea.\n",
        "\n",
        "\n",
        "\n",
        "Now enough said , let's see how to implement this. Since the dataset we used before did not have userId(which is necessary for collaborative filtering) let's load another dataset. We'll be using the Surprise library to implement SVD.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PItRjy4PXhZV"
      },
      "outputs": [],
      "source": [
        "from surprise import Reader, Dataset, SVD, evaluate\n",
        "reader = Reader()\n",
        "ratings = pd.read_csv('../input/the-movies-dataset/ratings_small.csv')\n",
        "ratings.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4j_B-_iXjyo"
      },
      "outputs": [],
      "source": [
        "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
        "data.split(n_folds=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FdItPsUXms9"
      },
      "outputs": [],
      "source": [
        "svd = SVD()\n",
        "evaluate(svd, data, measures=['RMSE', 'MAE'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVjzTh2oXo1Q"
      },
      "outputs": [],
      "source": [
        "trainset = data.build_full_trainset()\n",
        "svd.fit(trainset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVhs3qrpXrL3"
      },
      "outputs": [],
      "source": [
        "ratings[ratings['userId'] == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGv6hVf4Xska"
      },
      "outputs": [],
      "source": [
        "svd.predict(1, 302, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBBU83vjXuOf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "We create recommenders using demographic , content- based and collaborative filtering.\n",
        " While demographic filtering is very elemantary and cannot be used practically, \n",
        " Hybrid Systems can take advantage of content-based and collaborative filtering as the two approaches are proved to be almost complimentary.\n",
        "  This model was very baseline and only provides a fundamental framework to start with.\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIeMPg63hMzwSGxPjjXOma",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}